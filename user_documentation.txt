```markdown
# S3 File Processing and PII Obfuscation Module

This Python module provides functions for reading files from Amazon S3, obfuscating Personally Identifiable Information (PII), and writing processed data back as byte streams.  It supports CSV, JSON (lines delimited), and Parquet file formats.

## Installation

```bash
pip install boto3 pandas pyarrow
```

## Functions

### `read_file_from_s3(bucket_name: str, object_key: str, file_format: str) -> pd.DataFrame`

Reads a file from an S3 bucket and returns it as a pandas DataFrame.

**Parameters:**

*   `bucket_name` (str): The name of the S3 bucket.
*   `object_key` (str): The key of the object in the S3 bucket.
*   `file_format` (str): The format of the file ("csv", "json", or "parquet").

**Returns:**

*   `pd.DataFrame`: The DataFrame containing the data from the S3 file. Returns an empty DataFrame if the file is empty or if there is a parsing error.

**Raises:**

*   `ValueError`: If an unsupported file format is provided.
*   `RuntimeError`: If there is an error reading the file from S3.

**Example:**

```python
import pandas as pd
from your_module import read_file_from_s3  # Replace your_module

df = read_file_from_s3("my-bucket", "data.csv", "csv")
print(df.head())
```

### `obfuscate_pii(dataframe: pd.DataFrame, pii_fields: list) -> pd.DataFrame`

Replaces specified PII fields in a DataFrame with "***".

**Parameters:**

*   `dataframe` (pd.DataFrame): The input DataFrame.
*   `pii_fields` (list): A list of column names representing PII fields.

**Returns:**

*   `pd.DataFrame`: A new DataFrame with the PII fields obfuscated.  The original DataFrame is not modified.

**Example:**

```python
from your_module import obfuscate_pii

obfuscated_df = obfuscate_pii(df, ["name", "email"])
print(obfuscated_df.head())
```

### `write_to_bytes(dataframe: pd.DataFrame, file_format: str) -> io.BytesIO`

Converts a DataFrame to a byte stream in the specified format.

**Parameters:**

*   `dataframe` (pd.DataFrame): The input DataFrame.
*   `file_format` (str): The desired output format ("csv", "json", or "parquet").

**Returns:**

*   `io.BytesIO`: A byte stream containing the data in the specified format.

**Raises:**

*   `ValueError`: If an unsupported output format is provided.
*   `RuntimeError`: If there is an error writing to the byte stream.

**Example:**

```python
from your_module import write_to_bytes

csv_buffer = write_to_bytes(obfuscated_df, "csv")
# You can now use csv_buffer (e.g., upload to S3)
```

### `process_s3_file(json_input: str) -> io.BytesIO`

Processes a file from S3, obfuscates PII fields, and returns the result as a byte stream.  This function orchestrates the other functions in the module.

**Parameters:**

*   `json_input` (str): A JSON string containing the S3 URI and PII fields to obfuscate.  The JSON should have the following structure:

```json
{
  "file_to_obfuscate": "s3://my-bucket/data.csv",
  "pii_fields": ["name", "email"]
}
```

**Returns:**

*   `io.BytesIO`: A byte stream containing the processed data (CSV, JSON or Parquet).

**Raises:**

*   `ValueError`: If the JSON input is invalid or missing required fields, or if an unsupported output format is requested.
*   `RuntimeError`: If there is an error reading from S3 or processing the file.

**Example:**

```python
from your_module import process_s3_file

json_input = """
{
  "file_to_obfuscate": "s3://my-bucket/data.csv",
  "pii_fields": ["name", "email"]
}
"""

processed_data = process_s3_file(json_input)

# Use processed_data (e.g., save to a file, upload to another S3 location)

with open("obfuscated_data.csv", "wb") as f:
    f.write(processed_data.getvalue())
```

## Error Handling

The module uses exceptions to handle errors.  `ValueError` is raised for invalid input (e.g., unsupported file formats), and `RuntimeError` is raised for errors during S3 interaction or file processing.  The `read_file_from_s3` method will return an empty DataFrame if a file is empty or there is a parsing error to avoid breaking the pipeline.

## Dependencies

*   `boto3`: For interacting with Amazon S3.
*   `pandas`: For working with DataFrames.
*   `pyarrow`: For reading and writing Parquet files.
*   `io`: For working with in-memory byte streams.
*   `json`: For parsing JSON input.

## AWS Credentials

When running on AWS (EC2, Lambda, etc.), the module will automatically use IAM roles.  For local development, configure your AWS credentials using the AWS credentials file (`~/.aws/credentials`) or environment variables.  **Never** hardcode AWS credentials in your code.

## Example Usage (End-to-End)

1.  **Prepare your JSON input:** Create a JSON string with the S3 URI of the file you want to process and the list of PII fields.
2.  **Call `process_s3_file`:** Pass the JSON string to the `process_s3_file` function.
3.  **Use the byte stream:** The function returns a `io.BytesIO` object. You can use this object to save the processed data to a file, upload it to another S3 location, or perform other operations.

This documentation should help users understand and use your module effectively.  Remember to replace `your_module` with the actual name of your Python module.
